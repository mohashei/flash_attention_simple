# flash_attention_simple
A simple non-optimal flash attention implementation. Relies on C++ and CUDA without python.
